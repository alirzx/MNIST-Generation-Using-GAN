{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490e6f6f-a524-4358-94fa-5ef81e809747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae741f8e-cdd5-43ed-98e6-0958c788a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 50\n",
    "latent_dim = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae7fcd5-695a-46ed-94b5-501884b5baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c1031d-3436-4971-abf5-004646a1620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loading and Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef535ef-dfe5-4abf-ba46-939fdd238434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d423156e-6958-41fa-b7d9-a8b64247326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_samples(dataset):\n",
    "    # Create a figure to display the samples\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Dictionary to hold one sample image for each label\n",
    "    samples = {i: None for i in range(10)}\n",
    "    \n",
    "    # Iterate through the dataset to find one sample for each label\n",
    "    for img, label in dataset:\n",
    "        if samples[label] is None:  # No need for .item() here\n",
    "            samples[label] = img\n",
    "            \n",
    "        # Stop once we have one sample for each label\n",
    "        if None not in samples.values():  # Check if all samples are filled\n",
    "            break\n",
    "\n",
    "    # Display the samples\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(samples[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Label: {i}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples from the dataset\n",
    "show_samples(mnist_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20128845-10e9-49a5-8fcc-70eedf81fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 28 * 28),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(-1, 1, 28, 28)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()  # Output probability\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ec8c9c-e782-4f24-ad6c-512cb21e5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss Function and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2e8d1-ba66-4f2d-bc8c-5e55e8ef1e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e158a4-5b98-412a-aef6-f417d652bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "def train_gan(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # Create labels\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            outputs = discriminator(real_imgs)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            outputs = discriminator(fake_imgs.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            outputs = discriminator(fake_imgs)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        # Save generated images at the end of each epoch\n",
    "        save_generated_images(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7f8b7d-b9e9-4b2c-9c84-4995c6fa4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/938], D Loss: 0.4518, G Loss: 1.7374\n",
      "Epoch [1/50], Step [200/938], D Loss: 0.3889, G Loss: 1.4849\n",
      "Epoch [1/50], Step [300/938], D Loss: 0.3866, G Loss: 7.4824\n",
      "Epoch [1/50], Step [400/938], D Loss: 1.0611, G Loss: 5.2626\n",
      "Epoch [1/50], Step [500/938], D Loss: 0.3375, G Loss: 9.3940\n",
      "Epoch [1/50], Step [600/938], D Loss: 0.0185, G Loss: 6.5391\n",
      "Epoch [1/50], Step [700/938], D Loss: 0.0097, G Loss: 5.6113\n",
      "Epoch [1/50], Step [800/938], D Loss: 0.0073, G Loss: 6.0440\n",
      "Epoch [1/50], Step [900/938], D Loss: 0.0254, G Loss: 8.8203\n",
      "Epoch [2/50], Step [100/938], D Loss: 0.0209, G Loss: 7.1296\n",
      "Epoch [2/50], Step [200/938], D Loss: 0.2379, G Loss: 12.2219\n",
      "Epoch [2/50], Step [300/938], D Loss: 0.1093, G Loss: 15.4080\n",
      "Epoch [2/50], Step [400/938], D Loss: 0.1568, G Loss: 7.4985\n",
      "Epoch [2/50], Step [500/938], D Loss: 0.7653, G Loss: 14.2647\n",
      "Epoch [2/50], Step [600/938], D Loss: 1.0896, G Loss: 7.6979\n",
      "Epoch [2/50], Step [700/938], D Loss: 0.2796, G Loss: 6.4058\n",
      "Epoch [2/50], Step [800/938], D Loss: 0.2124, G Loss: 3.4154\n",
      "Epoch [2/50], Step [900/938], D Loss: 1.1341, G Loss: 2.3416\n",
      "Epoch [3/50], Step [100/938], D Loss: 1.1914, G Loss: 2.2106\n",
      "Epoch [3/50], Step [200/938], D Loss: 0.7790, G Loss: 3.0918\n",
      "Epoch [3/50], Step [300/938], D Loss: 1.4572, G Loss: 3.7498\n",
      "Epoch [3/50], Step [400/938], D Loss: 1.3436, G Loss: 3.6602\n",
      "Epoch [3/50], Step [500/938], D Loss: 1.1720, G Loss: 1.8006\n",
      "Epoch [3/50], Step [600/938], D Loss: 0.4613, G Loss: 3.3683\n",
      "Epoch [3/50], Step [700/938], D Loss: 0.4348, G Loss: 3.7240\n",
      "Epoch [3/50], Step [800/938], D Loss: 0.0505, G Loss: 4.1125\n",
      "Epoch [3/50], Step [900/938], D Loss: 0.6491, G Loss: 3.4869\n",
      "Epoch [4/50], Step [100/938], D Loss: 0.2824, G Loss: 4.3553\n",
      "Epoch [4/50], Step [200/938], D Loss: 0.0510, G Loss: 5.1174\n",
      "Epoch [4/50], Step [300/938], D Loss: 0.2267, G Loss: 4.3741\n",
      "Epoch [4/50], Step [400/938], D Loss: 0.5403, G Loss: 4.5405\n",
      "Epoch [4/50], Step [500/938], D Loss: 0.0910, G Loss: 5.7072\n",
      "Epoch [4/50], Step [600/938], D Loss: 0.1381, G Loss: 3.2009\n",
      "Epoch [4/50], Step [700/938], D Loss: 0.2594, G Loss: 3.8894\n",
      "Epoch [4/50], Step [800/938], D Loss: 0.1071, G Loss: 4.3311\n",
      "Epoch [4/50], Step [900/938], D Loss: 0.1421, G Loss: 4.4311\n",
      "Epoch [5/50], Step [100/938], D Loss: 0.3873, G Loss: 3.4574\n",
      "Epoch [5/50], Step [200/938], D Loss: 0.1692, G Loss: 4.4894\n",
      "Epoch [5/50], Step [300/938], D Loss: 0.5439, G Loss: 5.5570\n",
      "Epoch [5/50], Step [400/938], D Loss: 0.1318, G Loss: 5.2046\n",
      "Epoch [5/50], Step [500/938], D Loss: 0.2585, G Loss: 4.7347\n",
      "Epoch [5/50], Step [600/938], D Loss: 0.2297, G Loss: 6.4284\n",
      "Epoch [5/50], Step [700/938], D Loss: 0.1241, G Loss: 4.7792\n",
      "Epoch [5/50], Step [800/938], D Loss: 0.0909, G Loss: 7.8387\n",
      "Epoch [5/50], Step [900/938], D Loss: 0.5470, G Loss: 4.2948\n",
      "Epoch [6/50], Step [100/938], D Loss: 0.1122, G Loss: 7.1844\n",
      "Epoch [6/50], Step [200/938], D Loss: 0.2298, G Loss: 6.4384\n",
      "Epoch [6/50], Step [300/938], D Loss: 0.3429, G Loss: 7.1318\n",
      "Epoch [6/50], Step [400/938], D Loss: 0.4088, G Loss: 7.5732\n",
      "Epoch [6/50], Step [500/938], D Loss: 0.3858, G Loss: 4.4665\n",
      "Epoch [6/50], Step [600/938], D Loss: 0.3177, G Loss: 4.2432\n",
      "Epoch [6/50], Step [700/938], D Loss: 0.1424, G Loss: 4.6370\n",
      "Epoch [6/50], Step [800/938], D Loss: 0.1808, G Loss: 4.3905\n",
      "Epoch [6/50], Step [900/938], D Loss: 0.3104, G Loss: 5.3562\n",
      "Epoch [7/50], Step [100/938], D Loss: 0.0279, G Loss: 7.8254\n",
      "Epoch [7/50], Step [200/938], D Loss: 0.1501, G Loss: 5.8708\n",
      "Epoch [7/50], Step [300/938], D Loss: 0.2895, G Loss: 2.5194\n",
      "Epoch [7/50], Step [400/938], D Loss: 0.4611, G Loss: 5.0672\n",
      "Epoch [7/50], Step [500/938], D Loss: 0.7039, G Loss: 3.5252\n",
      "Epoch [7/50], Step [600/938], D Loss: 0.3825, G Loss: 6.2015\n",
      "Epoch [7/50], Step [700/938], D Loss: 0.4289, G Loss: 6.0089\n",
      "Epoch [7/50], Step [800/938], D Loss: 0.0549, G Loss: 10.9348\n",
      "Epoch [7/50], Step [900/938], D Loss: 0.0428, G Loss: 5.6629\n",
      "Epoch [8/50], Step [100/938], D Loss: 0.1408, G Loss: 5.0846\n",
      "Epoch [8/50], Step [200/938], D Loss: 0.3102, G Loss: 5.2177\n",
      "Epoch [8/50], Step [300/938], D Loss: 0.3850, G Loss: 5.3347\n",
      "Epoch [8/50], Step [400/938], D Loss: 0.2761, G Loss: 6.5804\n",
      "Epoch [8/50], Step [500/938], D Loss: 0.1997, G Loss: 7.5853\n",
      "Epoch [8/50], Step [600/938], D Loss: 0.3938, G Loss: 4.1434\n",
      "Epoch [8/50], Step [700/938], D Loss: 0.2763, G Loss: 7.9183\n",
      "Epoch [8/50], Step [800/938], D Loss: 0.4936, G Loss: 6.2974\n",
      "Epoch [8/50], Step [900/938], D Loss: 0.3282, G Loss: 5.7070\n",
      "Epoch [9/50], Step [100/938], D Loss: 0.3257, G Loss: 3.6801\n",
      "Epoch [9/50], Step [200/938], D Loss: 0.0442, G Loss: 4.8313\n",
      "Epoch [9/50], Step [300/938], D Loss: 0.1590, G Loss: 3.3012\n",
      "Epoch [9/50], Step [400/938], D Loss: 0.4049, G Loss: 5.0214\n",
      "Epoch [9/50], Step [500/938], D Loss: 0.2455, G Loss: 8.2519\n",
      "Epoch [9/50], Step [600/938], D Loss: 0.2152, G Loss: 4.8523\n",
      "Epoch [9/50], Step [700/938], D Loss: 0.3102, G Loss: 3.6958\n",
      "Epoch [9/50], Step [800/938], D Loss: 0.3416, G Loss: 3.9039\n",
      "Epoch [9/50], Step [900/938], D Loss: 0.6098, G Loss: 2.7314\n",
      "Epoch [10/50], Step [100/938], D Loss: 0.4485, G Loss: 3.0135\n",
      "Epoch [10/50], Step [200/938], D Loss: 0.2605, G Loss: 3.9502\n",
      "Epoch [10/50], Step [300/938], D Loss: 0.4666, G Loss: 4.3072\n",
      "Epoch [10/50], Step [400/938], D Loss: 0.2687, G Loss: 4.9726\n",
      "Epoch [10/50], Step [500/938], D Loss: 0.2602, G Loss: 6.8866\n",
      "Epoch [10/50], Step [600/938], D Loss: 0.2364, G Loss: 6.0587\n",
      "Epoch [10/50], Step [700/938], D Loss: 0.2533, G Loss: 4.4923\n",
      "Epoch [10/50], Step [800/938], D Loss: 0.2332, G Loss: 4.4577\n",
      "Epoch [10/50], Step [900/938], D Loss: 0.3406, G Loss: 3.5676\n",
      "Epoch [11/50], Step [100/938], D Loss: 0.4537, G Loss: 4.4000\n",
      "Epoch [11/50], Step [200/938], D Loss: 0.2933, G Loss: 3.8347\n",
      "Epoch [11/50], Step [300/938], D Loss: 0.2706, G Loss: 5.2568\n",
      "Epoch [11/50], Step [400/938], D Loss: 0.4240, G Loss: 3.7562\n",
      "Epoch [11/50], Step [500/938], D Loss: 0.3653, G Loss: 4.1106\n",
      "Epoch [11/50], Step [600/938], D Loss: 0.2079, G Loss: 5.2039\n",
      "Epoch [11/50], Step [700/938], D Loss: 0.2293, G Loss: 3.4367\n",
      "Epoch [11/50], Step [800/938], D Loss: 0.2458, G Loss: 4.8215\n",
      "Epoch [11/50], Step [900/938], D Loss: 0.4623, G Loss: 4.6815\n",
      "Epoch [12/50], Step [100/938], D Loss: 0.2835, G Loss: 4.6094\n",
      "Epoch [12/50], Step [200/938], D Loss: 0.5629, G Loss: 3.9309\n",
      "Epoch [12/50], Step [300/938], D Loss: 0.1826, G Loss: 3.5775\n",
      "Epoch [12/50], Step [400/938], D Loss: 0.2473, G Loss: 3.5502\n",
      "Epoch [12/50], Step [500/938], D Loss: 0.1736, G Loss: 4.5131\n",
      "Epoch [12/50], Step [600/938], D Loss: 1.1015, G Loss: 4.6528\n",
      "Epoch [12/50], Step [700/938], D Loss: 0.4250, G Loss: 3.2395\n",
      "Epoch [12/50], Step [800/938], D Loss: 0.2386, G Loss: 5.0992\n",
      "Epoch [12/50], Step [900/938], D Loss: 0.3466, G Loss: 4.8694\n",
      "Epoch [13/50], Step [100/938], D Loss: 0.3638, G Loss: 2.7293\n",
      "Epoch [13/50], Step [200/938], D Loss: 0.3072, G Loss: 4.8405\n",
      "Epoch [13/50], Step [300/938], D Loss: 0.4971, G Loss: 4.3204\n",
      "Epoch [13/50], Step [400/938], D Loss: 0.5668, G Loss: 4.3662\n",
      "Epoch [13/50], Step [500/938], D Loss: 0.4282, G Loss: 3.6004\n",
      "Epoch [13/50], Step [600/938], D Loss: 0.4610, G Loss: 2.8969\n",
      "Epoch [13/50], Step [700/938], D Loss: 0.1570, G Loss: 4.5751\n",
      "Epoch [13/50], Step [800/938], D Loss: 0.2845, G Loss: 5.8741\n",
      "Epoch [13/50], Step [900/938], D Loss: 0.2647, G Loss: 3.9160\n",
      "Epoch [14/50], Step [100/938], D Loss: 0.2105, G Loss: 5.5156\n",
      "Epoch [14/50], Step [200/938], D Loss: 0.3462, G Loss: 3.1970\n",
      "Epoch [14/50], Step [300/938], D Loss: 0.3694, G Loss: 3.3765\n",
      "Epoch [14/50], Step [400/938], D Loss: 0.4854, G Loss: 2.7887\n",
      "Epoch [14/50], Step [500/938], D Loss: 0.3414, G Loss: 4.2037\n",
      "Epoch [14/50], Step [600/938], D Loss: 0.5846, G Loss: 2.7826\n",
      "Epoch [14/50], Step [700/938], D Loss: 0.2828, G Loss: 3.5549\n",
      "Epoch [14/50], Step [800/938], D Loss: 0.3163, G Loss: 4.2437\n",
      "Epoch [14/50], Step [900/938], D Loss: 0.2645, G Loss: 4.6316\n",
      "Epoch [15/50], Step [100/938], D Loss: 0.3376, G Loss: 3.8820\n",
      "Epoch [15/50], Step [200/938], D Loss: 0.5148, G Loss: 5.0326\n",
      "Epoch [15/50], Step [300/938], D Loss: 0.2849, G Loss: 4.3269\n",
      "Epoch [15/50], Step [400/938], D Loss: 0.2409, G Loss: 4.1750\n",
      "Epoch [15/50], Step [500/938], D Loss: 0.4624, G Loss: 5.7903\n",
      "Epoch [15/50], Step [600/938], D Loss: 0.3941, G Loss: 3.7436\n",
      "Epoch [15/50], Step [700/938], D Loss: 0.1305, G Loss: 4.3146\n",
      "Epoch [15/50], Step [800/938], D Loss: 0.1813, G Loss: 3.8452\n",
      "Epoch [15/50], Step [900/938], D Loss: 0.3396, G Loss: 3.7097\n",
      "Epoch [16/50], Step [100/938], D Loss: 0.4339, G Loss: 2.6455\n",
      "Epoch [16/50], Step [200/938], D Loss: 0.4674, G Loss: 3.1673\n",
      "Epoch [16/50], Step [300/938], D Loss: 0.4115, G Loss: 3.5007\n",
      "Epoch [16/50], Step [400/938], D Loss: 0.3621, G Loss: 4.2064\n",
      "Epoch [16/50], Step [500/938], D Loss: 0.3192, G Loss: 3.0321\n",
      "Epoch [16/50], Step [600/938], D Loss: 0.3808, G Loss: 4.0656\n",
      "Epoch [16/50], Step [700/938], D Loss: 0.3385, G Loss: 2.8361\n",
      "Epoch [16/50], Step [800/938], D Loss: 0.2530, G Loss: 3.7577\n",
      "Epoch [16/50], Step [900/938], D Loss: 0.3821, G Loss: 3.1956\n",
      "Epoch [17/50], Step [100/938], D Loss: 0.4365, G Loss: 4.3890\n",
      "Epoch [17/50], Step [200/938], D Loss: 0.3404, G Loss: 4.0167\n",
      "Epoch [17/50], Step [300/938], D Loss: 0.4617, G Loss: 4.0782\n",
      "Epoch [17/50], Step [400/938], D Loss: 0.2538, G Loss: 4.0365\n",
      "Epoch [17/50], Step [500/938], D Loss: 0.3120, G Loss: 4.3821\n",
      "Epoch [17/50], Step [600/938], D Loss: 0.2949, G Loss: 3.5092\n",
      "Epoch [17/50], Step [700/938], D Loss: 0.4577, G Loss: 2.4274\n",
      "Epoch [17/50], Step [800/938], D Loss: 0.3333, G Loss: 3.8830\n",
      "Epoch [17/50], Step [900/938], D Loss: 0.3304, G Loss: 4.8419\n",
      "Epoch [18/50], Step [100/938], D Loss: 0.3557, G Loss: 3.7150\n",
      "Epoch [18/50], Step [200/938], D Loss: 0.4075, G Loss: 4.2005\n",
      "Epoch [18/50], Step [300/938], D Loss: 0.3012, G Loss: 3.4927\n",
      "Epoch [18/50], Step [400/938], D Loss: 0.2052, G Loss: 2.7562\n",
      "Epoch [18/50], Step [500/938], D Loss: 0.5433, G Loss: 3.0857\n",
      "Epoch [18/50], Step [600/938], D Loss: 0.3984, G Loss: 3.7453\n",
      "Epoch [18/50], Step [700/938], D Loss: 0.3557, G Loss: 2.9578\n",
      "Epoch [18/50], Step [800/938], D Loss: 0.5092, G Loss: 3.8687\n",
      "Epoch [18/50], Step [900/938], D Loss: 0.2409, G Loss: 4.6075\n",
      "Epoch [19/50], Step [100/938], D Loss: 0.4766, G Loss: 3.2171\n",
      "Epoch [19/50], Step [200/938], D Loss: 0.2575, G Loss: 3.5348\n",
      "Epoch [19/50], Step [300/938], D Loss: 0.3602, G Loss: 4.0939\n",
      "Epoch [19/50], Step [400/938], D Loss: 0.4195, G Loss: 3.1024\n",
      "Epoch [19/50], Step [500/938], D Loss: 0.2603, G Loss: 2.3860\n",
      "Epoch [19/50], Step [600/938], D Loss: 0.3499, G Loss: 3.0066\n",
      "Epoch [19/50], Step [700/938], D Loss: 0.9923, G Loss: 3.0443\n",
      "Epoch [19/50], Step [800/938], D Loss: 0.3901, G Loss: 3.7849\n",
      "Epoch [19/50], Step [900/938], D Loss: 0.2160, G Loss: 3.7188\n",
      "Epoch [20/50], Step [100/938], D Loss: 0.3142, G Loss: 3.9223\n",
      "Epoch [20/50], Step [200/938], D Loss: 0.3712, G Loss: 3.6017\n",
      "Epoch [20/50], Step [300/938], D Loss: 0.4341, G Loss: 3.0071\n",
      "Epoch [20/50], Step [400/938], D Loss: 0.5189, G Loss: 4.1987\n",
      "Epoch [20/50], Step [500/938], D Loss: 0.3607, G Loss: 3.5025\n",
      "Epoch [20/50], Step [600/938], D Loss: 0.2946, G Loss: 4.9764\n",
      "Epoch [20/50], Step [700/938], D Loss: 0.3995, G Loss: 2.6708\n",
      "Epoch [20/50], Step [800/938], D Loss: 0.2905, G Loss: 3.5469\n",
      "Epoch [20/50], Step [900/938], D Loss: 0.3811, G Loss: 4.6080\n",
      "Epoch [21/50], Step [100/938], D Loss: 0.5035, G Loss: 2.6293\n",
      "Epoch [21/50], Step [200/938], D Loss: 0.6171, G Loss: 3.1374\n",
      "Epoch [21/50], Step [300/938], D Loss: 0.3235, G Loss: 2.4842\n",
      "Epoch [21/50], Step [400/938], D Loss: 0.4049, G Loss: 4.0399\n",
      "Epoch [21/50], Step [500/938], D Loss: 0.4037, G Loss: 2.6708\n",
      "Epoch [21/50], Step [600/938], D Loss: 0.4161, G Loss: 3.0972\n",
      "Epoch [21/50], Step [700/938], D Loss: 0.4666, G Loss: 2.8983\n",
      "Epoch [21/50], Step [800/938], D Loss: 0.4408, G Loss: 3.6131\n",
      "Epoch [21/50], Step [900/938], D Loss: 0.6721, G Loss: 5.5049\n",
      "Epoch [22/50], Step [100/938], D Loss: 0.4143, G Loss: 2.6408\n",
      "Epoch [22/50], Step [200/938], D Loss: 0.5551, G Loss: 4.0951\n",
      "Epoch [22/50], Step [300/938], D Loss: 0.2785, G Loss: 4.3377\n",
      "Epoch [22/50], Step [400/938], D Loss: 0.5604, G Loss: 2.2880\n",
      "Epoch [22/50], Step [500/938], D Loss: 0.6744, G Loss: 2.5473\n",
      "Epoch [22/50], Step [600/938], D Loss: 0.3269, G Loss: 2.8906\n",
      "Epoch [22/50], Step [700/938], D Loss: 0.5157, G Loss: 4.0531\n",
      "Epoch [22/50], Step [800/938], D Loss: 0.5091, G Loss: 2.4658\n",
      "Epoch [22/50], Step [900/938], D Loss: 0.4841, G Loss: 4.1022\n",
      "Epoch [23/50], Step [100/938], D Loss: 0.3980, G Loss: 3.3014\n",
      "Epoch [23/50], Step [200/938], D Loss: 0.5374, G Loss: 4.5704\n",
      "Epoch [23/50], Step [300/938], D Loss: 0.4150, G Loss: 3.3863\n",
      "Epoch [23/50], Step [400/938], D Loss: 0.4902, G Loss: 2.7614\n",
      "Epoch [23/50], Step [500/938], D Loss: 0.4541, G Loss: 2.8188\n",
      "Epoch [23/50], Step [600/938], D Loss: 0.4655, G Loss: 3.1335\n",
      "Epoch [23/50], Step [700/938], D Loss: 0.6183, G Loss: 3.4388\n",
      "Epoch [23/50], Step [800/938], D Loss: 0.4953, G Loss: 2.7109\n",
      "Epoch [23/50], Step [900/938], D Loss: 0.3848, G Loss: 3.3180\n",
      "Epoch [24/50], Step [100/938], D Loss: 0.3945, G Loss: 4.0591\n",
      "Epoch [24/50], Step [200/938], D Loss: 0.5537, G Loss: 3.5765\n",
      "Epoch [24/50], Step [300/938], D Loss: 0.5660, G Loss: 2.2527\n",
      "Epoch [24/50], Step [400/938], D Loss: 0.4516, G Loss: 2.8947\n",
      "Epoch [24/50], Step [500/938], D Loss: 0.7998, G Loss: 3.3838\n",
      "Epoch [24/50], Step [600/938], D Loss: 0.5834, G Loss: 2.7307\n",
      "Epoch [24/50], Step [700/938], D Loss: 0.4620, G Loss: 3.0247\n",
      "Epoch [24/50], Step [800/938], D Loss: 0.4351, G Loss: 2.5061\n",
      "Epoch [24/50], Step [900/938], D Loss: 0.5865, G Loss: 2.1886\n",
      "Epoch [25/50], Step [100/938], D Loss: 0.4193, G Loss: 3.0990\n",
      "Epoch [25/50], Step [200/938], D Loss: 0.5450, G Loss: 2.5717\n",
      "Epoch [25/50], Step [300/938], D Loss: 0.5655, G Loss: 2.4143\n",
      "Epoch [25/50], Step [400/938], D Loss: 0.6056, G Loss: 2.2743\n",
      "Epoch [25/50], Step [500/938], D Loss: 0.5220, G Loss: 2.4727\n",
      "Epoch [25/50], Step [600/938], D Loss: 0.6138, G Loss: 2.3451\n",
      "Epoch [25/50], Step [700/938], D Loss: 0.5988, G Loss: 2.2952\n",
      "Epoch [25/50], Step [800/938], D Loss: 0.7060, G Loss: 2.7463\n",
      "Epoch [25/50], Step [900/938], D Loss: 0.5691, G Loss: 2.6954\n",
      "Epoch [26/50], Step [100/938], D Loss: 0.5742, G Loss: 2.0456\n",
      "Epoch [26/50], Step [200/938], D Loss: 0.6433, G Loss: 3.6875\n",
      "Epoch [26/50], Step [300/938], D Loss: 0.4535, G Loss: 2.6728\n",
      "Epoch [26/50], Step [400/938], D Loss: 0.6302, G Loss: 2.0432\n",
      "Epoch [26/50], Step [500/938], D Loss: 0.6163, G Loss: 2.7694\n",
      "Epoch [26/50], Step [600/938], D Loss: 0.4482, G Loss: 2.4260\n",
      "Epoch [26/50], Step [700/938], D Loss: 0.8594, G Loss: 2.1033\n",
      "Epoch [26/50], Step [800/938], D Loss: 0.7727, G Loss: 2.6259\n",
      "Epoch [26/50], Step [900/938], D Loss: 0.3903, G Loss: 2.6628\n",
      "Epoch [27/50], Step [100/938], D Loss: 0.5486, G Loss: 4.0319\n",
      "Epoch [27/50], Step [200/938], D Loss: 0.5678, G Loss: 2.5151\n",
      "Epoch [27/50], Step [300/938], D Loss: 0.9329, G Loss: 2.7952\n",
      "Epoch [27/50], Step [400/938], D Loss: 0.5382, G Loss: 2.7883\n",
      "Epoch [27/50], Step [500/938], D Loss: 0.6676, G Loss: 2.7698\n",
      "Epoch [27/50], Step [600/938], D Loss: 0.6016, G Loss: 2.7835\n",
      "Epoch [27/50], Step [700/938], D Loss: 0.4268, G Loss: 2.1949\n",
      "Epoch [27/50], Step [800/938], D Loss: 0.7354, G Loss: 3.1549\n",
      "Epoch [27/50], Step [900/938], D Loss: 0.5856, G Loss: 2.5295\n",
      "Epoch [28/50], Step [100/938], D Loss: 0.7156, G Loss: 2.3877\n",
      "Epoch [28/50], Step [200/938], D Loss: 0.5329, G Loss: 3.1567\n",
      "Epoch [28/50], Step [300/938], D Loss: 0.4862, G Loss: 2.7526\n",
      "Epoch [28/50], Step [400/938], D Loss: 0.4730, G Loss: 2.3094\n",
      "Epoch [28/50], Step [500/938], D Loss: 0.4819, G Loss: 3.5058\n",
      "Epoch [28/50], Step [600/938], D Loss: 0.5523, G Loss: 1.6177\n",
      "Epoch [28/50], Step [700/938], D Loss: 0.4270, G Loss: 3.6440\n",
      "Epoch [28/50], Step [800/938], D Loss: 0.7817, G Loss: 3.5159\n",
      "Epoch [28/50], Step [900/938], D Loss: 0.5000, G Loss: 2.2624\n",
      "Epoch [29/50], Step [100/938], D Loss: 0.6559, G Loss: 2.0442\n",
      "Epoch [29/50], Step [200/938], D Loss: 0.8562, G Loss: 2.3425\n",
      "Epoch [29/50], Step [300/938], D Loss: 0.7235, G Loss: 2.4657\n",
      "Epoch [29/50], Step [400/938], D Loss: 0.5244, G Loss: 2.7986\n",
      "Epoch [29/50], Step [500/938], D Loss: 0.6960, G Loss: 3.6534\n",
      "Epoch [29/50], Step [600/938], D Loss: 0.5506, G Loss: 2.7844\n",
      "Epoch [29/50], Step [700/938], D Loss: 0.7557, G Loss: 3.4586\n",
      "Epoch [29/50], Step [800/938], D Loss: 0.5248, G Loss: 2.3264\n",
      "Epoch [29/50], Step [900/938], D Loss: 0.3738, G Loss: 2.6902\n",
      "Epoch [30/50], Step [100/938], D Loss: 0.7921, G Loss: 1.8790\n",
      "Epoch [30/50], Step [200/938], D Loss: 0.6028, G Loss: 2.3644\n",
      "Epoch [30/50], Step [300/938], D Loss: 0.6358, G Loss: 2.9690\n",
      "Epoch [30/50], Step [400/938], D Loss: 0.8277, G Loss: 1.8203\n",
      "Epoch [30/50], Step [500/938], D Loss: 0.6526, G Loss: 2.7724\n",
      "Epoch [30/50], Step [600/938], D Loss: 0.4640, G Loss: 2.3095\n",
      "Epoch [30/50], Step [700/938], D Loss: 0.5862, G Loss: 2.3780\n",
      "Epoch [30/50], Step [800/938], D Loss: 0.4841, G Loss: 2.5320\n",
      "Epoch [30/50], Step [900/938], D Loss: 0.7716, G Loss: 2.8716\n",
      "Epoch [31/50], Step [100/938], D Loss: 0.5406, G Loss: 2.2406\n",
      "Epoch [31/50], Step [200/938], D Loss: 0.4901, G Loss: 2.1137\n",
      "Epoch [31/50], Step [300/938], D Loss: 0.7055, G Loss: 2.0709\n",
      "Epoch [31/50], Step [400/938], D Loss: 0.7920, G Loss: 1.6840\n",
      "Epoch [31/50], Step [500/938], D Loss: 0.8735, G Loss: 1.3514\n",
      "Epoch [31/50], Step [600/938], D Loss: 0.6794, G Loss: 3.1578\n",
      "Epoch [31/50], Step [700/938], D Loss: 0.6905, G Loss: 1.8911\n",
      "Epoch [31/50], Step [800/938], D Loss: 0.4020, G Loss: 3.0993\n",
      "Epoch [31/50], Step [900/938], D Loss: 0.5068, G Loss: 2.4719\n",
      "Epoch [32/50], Step [100/938], D Loss: 0.6691, G Loss: 2.1901\n",
      "Epoch [32/50], Step [200/938], D Loss: 0.8752, G Loss: 2.9929\n",
      "Epoch [32/50], Step [300/938], D Loss: 0.5964, G Loss: 2.4786\n",
      "Epoch [32/50], Step [400/938], D Loss: 0.8892, G Loss: 2.2329\n",
      "Epoch [32/50], Step [500/938], D Loss: 1.1547, G Loss: 1.6765\n",
      "Epoch [32/50], Step [600/938], D Loss: 0.7007, G Loss: 2.1256\n",
      "Epoch [32/50], Step [700/938], D Loss: 0.5498, G Loss: 2.3210\n",
      "Epoch [32/50], Step [800/938], D Loss: 0.6088, G Loss: 2.5223\n",
      "Epoch [32/50], Step [900/938], D Loss: 0.7328, G Loss: 3.0494\n",
      "Epoch [33/50], Step [100/938], D Loss: 0.7322, G Loss: 2.0505\n",
      "Epoch [33/50], Step [200/938], D Loss: 0.4965, G Loss: 2.2303\n",
      "Epoch [33/50], Step [300/938], D Loss: 0.4893, G Loss: 3.3181\n",
      "Epoch [33/50], Step [400/938], D Loss: 0.4775, G Loss: 2.5309\n",
      "Epoch [33/50], Step [500/938], D Loss: 0.6806, G Loss: 2.6367\n",
      "Epoch [33/50], Step [600/938], D Loss: 0.6650, G Loss: 2.7129\n",
      "Epoch [33/50], Step [700/938], D Loss: 0.4450, G Loss: 2.9456\n",
      "Epoch [33/50], Step [800/938], D Loss: 0.6363, G Loss: 2.1496\n",
      "Epoch [33/50], Step [900/938], D Loss: 0.5615, G Loss: 2.7471\n",
      "Epoch [34/50], Step [100/938], D Loss: 0.4407, G Loss: 3.0070\n",
      "Epoch [34/50], Step [200/938], D Loss: 0.5277, G Loss: 2.5500\n",
      "Epoch [34/50], Step [300/938], D Loss: 0.5627, G Loss: 3.0054\n",
      "Epoch [34/50], Step [400/938], D Loss: 0.6659, G Loss: 2.0738\n",
      "Epoch [34/50], Step [500/938], D Loss: 0.5910, G Loss: 1.7763\n",
      "Epoch [34/50], Step [600/938], D Loss: 0.8082, G Loss: 2.1276\n",
      "Epoch [34/50], Step [700/938], D Loss: 0.4576, G Loss: 2.6680\n",
      "Epoch [34/50], Step [800/938], D Loss: 0.7002, G Loss: 1.8360\n",
      "Epoch [34/50], Step [900/938], D Loss: 0.5340, G Loss: 3.2775\n",
      "Epoch [35/50], Step [100/938], D Loss: 0.7064, G Loss: 2.4567\n",
      "Epoch [35/50], Step [200/938], D Loss: 0.5815, G Loss: 2.6376\n",
      "Epoch [35/50], Step [300/938], D Loss: 0.6472, G Loss: 2.6351\n",
      "Epoch [35/50], Step [400/938], D Loss: 0.8663, G Loss: 2.2266\n",
      "Epoch [35/50], Step [500/938], D Loss: 0.6745, G Loss: 3.1226\n",
      "Epoch [35/50], Step [600/938], D Loss: 0.6180, G Loss: 1.4772\n",
      "Epoch [35/50], Step [700/938], D Loss: 1.0885, G Loss: 2.4395\n",
      "Epoch [35/50], Step [800/938], D Loss: 0.4304, G Loss: 1.6802\n",
      "Epoch [35/50], Step [900/938], D Loss: 0.7027, G Loss: 2.4208\n",
      "Epoch [36/50], Step [100/938], D Loss: 0.6708, G Loss: 2.2432\n",
      "Epoch [36/50], Step [200/938], D Loss: 0.5687, G Loss: 2.0166\n",
      "Epoch [36/50], Step [300/938], D Loss: 0.7868, G Loss: 1.6406\n",
      "Epoch [36/50], Step [400/938], D Loss: 0.5307, G Loss: 1.9899\n",
      "Epoch [36/50], Step [500/938], D Loss: 0.9130, G Loss: 2.1797\n",
      "Epoch [36/50], Step [600/938], D Loss: 0.7685, G Loss: 1.7162\n",
      "Epoch [36/50], Step [700/938], D Loss: 0.6824, G Loss: 2.1545\n",
      "Epoch [36/50], Step [800/938], D Loss: 0.4896, G Loss: 3.0296\n",
      "Epoch [36/50], Step [900/938], D Loss: 0.7431, G Loss: 3.0394\n",
      "Epoch [37/50], Step [100/938], D Loss: 0.9130, G Loss: 1.5847\n",
      "Epoch [37/50], Step [200/938], D Loss: 0.5167, G Loss: 2.4669\n",
      "Epoch [37/50], Step [300/938], D Loss: 0.7667, G Loss: 1.6579\n",
      "Epoch [37/50], Step [400/938], D Loss: 0.4798, G Loss: 2.4701\n",
      "Epoch [37/50], Step [500/938], D Loss: 0.7583, G Loss: 1.4635\n",
      "Epoch [37/50], Step [600/938], D Loss: 0.7425, G Loss: 1.7385\n",
      "Epoch [37/50], Step [700/938], D Loss: 0.6233, G Loss: 1.9030\n",
      "Epoch [37/50], Step [800/938], D Loss: 0.6303, G Loss: 2.3907\n",
      "Epoch [37/50], Step [900/938], D Loss: 0.7206, G Loss: 1.7701\n",
      "Epoch [38/50], Step [100/938], D Loss: 0.5383, G Loss: 2.2159\n",
      "Epoch [38/50], Step [200/938], D Loss: 0.8709, G Loss: 1.8011\n",
      "Epoch [38/50], Step [300/938], D Loss: 0.5975, G Loss: 2.2680\n",
      "Epoch [38/50], Step [400/938], D Loss: 0.7772, G Loss: 2.1398\n",
      "Epoch [38/50], Step [500/938], D Loss: 0.5886, G Loss: 2.1020\n",
      "Epoch [38/50], Step [600/938], D Loss: 0.8908, G Loss: 2.3759\n",
      "Epoch [38/50], Step [700/938], D Loss: 0.6773, G Loss: 1.7848\n",
      "Epoch [38/50], Step [800/938], D Loss: 0.6047, G Loss: 1.8555\n",
      "Epoch [38/50], Step [900/938], D Loss: 0.9232, G Loss: 1.6318\n",
      "Epoch [39/50], Step [100/938], D Loss: 0.5874, G Loss: 2.3620\n",
      "Epoch [39/50], Step [200/938], D Loss: 1.0594, G Loss: 1.5897\n",
      "Epoch [39/50], Step [300/938], D Loss: 0.7292, G Loss: 2.1852\n",
      "Epoch [39/50], Step [400/938], D Loss: 0.5445, G Loss: 2.6861\n",
      "Epoch [39/50], Step [500/938], D Loss: 0.8309, G Loss: 1.8019\n",
      "Epoch [39/50], Step [600/938], D Loss: 0.5314, G Loss: 2.0976\n",
      "Epoch [39/50], Step [700/938], D Loss: 0.5170, G Loss: 1.9622\n",
      "Epoch [39/50], Step [800/938], D Loss: 0.8195, G Loss: 2.2643\n",
      "Epoch [39/50], Step [900/938], D Loss: 0.8539, G Loss: 1.4535\n",
      "Epoch [40/50], Step [100/938], D Loss: 0.5239, G Loss: 2.5169\n",
      "Epoch [40/50], Step [200/938], D Loss: 0.8499, G Loss: 2.2993\n",
      "Epoch [40/50], Step [300/938], D Loss: 0.8211, G Loss: 2.2392\n",
      "Epoch [40/50], Step [400/938], D Loss: 0.7053, G Loss: 2.1499\n",
      "Epoch [40/50], Step [500/938], D Loss: 0.7110, G Loss: 2.2232\n",
      "Epoch [40/50], Step [600/938], D Loss: 0.8691, G Loss: 1.5698\n",
      "Epoch [40/50], Step [700/938], D Loss: 0.5830, G Loss: 2.3322\n",
      "Epoch [40/50], Step [800/938], D Loss: 0.9302, G Loss: 2.0040\n",
      "Epoch [40/50], Step [900/938], D Loss: 0.7017, G Loss: 2.0310\n",
      "Epoch [41/50], Step [100/938], D Loss: 0.7163, G Loss: 2.0119\n",
      "Epoch [41/50], Step [200/938], D Loss: 0.8887, G Loss: 1.7309\n",
      "Epoch [41/50], Step [300/938], D Loss: 0.8920, G Loss: 1.7150\n",
      "Epoch [41/50], Step [400/938], D Loss: 0.7130, G Loss: 2.2529\n",
      "Epoch [41/50], Step [500/938], D Loss: 0.7466, G Loss: 1.8025\n",
      "Epoch [41/50], Step [600/938], D Loss: 0.9239, G Loss: 1.6237\n",
      "Epoch [41/50], Step [700/938], D Loss: 0.8539, G Loss: 1.9583\n",
      "Epoch [41/50], Step [800/938], D Loss: 0.6855, G Loss: 2.3676\n",
      "Epoch [41/50], Step [900/938], D Loss: 0.7512, G Loss: 1.7178\n",
      "Epoch [42/50], Step [100/938], D Loss: 0.7444, G Loss: 1.9956\n",
      "Epoch [42/50], Step [200/938], D Loss: 0.6105, G Loss: 2.6014\n",
      "Epoch [42/50], Step [300/938], D Loss: 0.8658, G Loss: 1.7439\n",
      "Epoch [42/50], Step [400/938], D Loss: 0.7708, G Loss: 2.2258\n",
      "Epoch [42/50], Step [500/938], D Loss: 0.7244, G Loss: 1.8573\n",
      "Epoch [42/50], Step [600/938], D Loss: 0.7101, G Loss: 2.2732\n",
      "Epoch [42/50], Step [700/938], D Loss: 0.6684, G Loss: 1.9710\n",
      "Epoch [42/50], Step [800/938], D Loss: 0.6524, G Loss: 2.1477\n",
      "Epoch [42/50], Step [900/938], D Loss: 0.6136, G Loss: 2.0977\n",
      "Epoch [43/50], Step [100/938], D Loss: 0.7454, G Loss: 1.3664\n",
      "Epoch [43/50], Step [200/938], D Loss: 0.9566, G Loss: 1.3302\n",
      "Epoch [43/50], Step [300/938], D Loss: 0.6633, G Loss: 1.8866\n",
      "Epoch [43/50], Step [400/938], D Loss: 0.7403, G Loss: 2.6932\n",
      "Epoch [43/50], Step [500/938], D Loss: 0.8081, G Loss: 2.5401\n",
      "Epoch [43/50], Step [600/938], D Loss: 0.5769, G Loss: 1.5137\n",
      "Epoch [43/50], Step [700/938], D Loss: 0.8315, G Loss: 1.8586\n",
      "Epoch [43/50], Step [800/938], D Loss: 0.9670, G Loss: 1.6997\n",
      "Epoch [43/50], Step [900/938], D Loss: 1.1990, G Loss: 1.9252\n",
      "Epoch [44/50], Step [100/938], D Loss: 0.8646, G Loss: 2.3599\n",
      "Epoch [44/50], Step [200/938], D Loss: 0.9132, G Loss: 2.2949\n",
      "Epoch [44/50], Step [300/938], D Loss: 0.8668, G Loss: 2.0586\n",
      "Epoch [44/50], Step [400/938], D Loss: 0.8898, G Loss: 1.6323\n",
      "Epoch [44/50], Step [500/938], D Loss: 0.8837, G Loss: 1.5878\n",
      "Epoch [44/50], Step [600/938], D Loss: 0.9466, G Loss: 2.0273\n",
      "Epoch [44/50], Step [700/938], D Loss: 0.9356, G Loss: 1.7334\n",
      "Epoch [44/50], Step [800/938], D Loss: 0.9070, G Loss: 2.1444\n",
      "Epoch [44/50], Step [900/938], D Loss: 0.7799, G Loss: 1.6291\n",
      "Epoch [45/50], Step [100/938], D Loss: 0.7300, G Loss: 2.0474\n",
      "Epoch [45/50], Step [200/938], D Loss: 0.8540, G Loss: 2.0321\n",
      "Epoch [45/50], Step [300/938], D Loss: 0.6580, G Loss: 2.0765\n",
      "Epoch [45/50], Step [400/938], D Loss: 0.8911, G Loss: 1.9550\n",
      "Epoch [45/50], Step [500/938], D Loss: 0.5627, G Loss: 2.0903\n",
      "Epoch [45/50], Step [600/938], D Loss: 0.5655, G Loss: 2.4353\n",
      "Epoch [45/50], Step [700/938], D Loss: 0.7543, G Loss: 1.2315\n",
      "Epoch [45/50], Step [800/938], D Loss: 1.1175, G Loss: 1.5140\n",
      "Epoch [45/50], Step [900/938], D Loss: 0.8880, G Loss: 2.4556\n",
      "Epoch [46/50], Step [100/938], D Loss: 0.9010, G Loss: 1.9972\n",
      "Epoch [46/50], Step [200/938], D Loss: 0.8151, G Loss: 1.9131\n",
      "Epoch [46/50], Step [300/938], D Loss: 0.6747, G Loss: 2.3405\n",
      "Epoch [46/50], Step [400/938], D Loss: 0.8785, G Loss: 2.0487\n",
      "Epoch [46/50], Step [500/938], D Loss: 0.8378, G Loss: 1.0952\n",
      "Epoch [46/50], Step [600/938], D Loss: 0.7942, G Loss: 1.8993\n",
      "Epoch [46/50], Step [700/938], D Loss: 0.9094, G Loss: 1.6006\n",
      "Epoch [46/50], Step [800/938], D Loss: 0.7767, G Loss: 2.0428\n",
      "Epoch [46/50], Step [900/938], D Loss: 0.8742, G Loss: 2.0707\n",
      "Epoch [47/50], Step [100/938], D Loss: 0.6082, G Loss: 2.2229\n",
      "Epoch [47/50], Step [200/938], D Loss: 1.0080, G Loss: 1.4585\n",
      "Epoch [47/50], Step [300/938], D Loss: 0.9067, G Loss: 1.6743\n",
      "Epoch [47/50], Step [400/938], D Loss: 1.2128, G Loss: 2.5114\n",
      "Epoch [47/50], Step [500/938], D Loss: 0.6779, G Loss: 2.6348\n",
      "Epoch [47/50], Step [600/938], D Loss: 0.6679, G Loss: 1.6351\n",
      "Epoch [47/50], Step [700/938], D Loss: 0.8866, G Loss: 1.6794\n",
      "Epoch [47/50], Step [800/938], D Loss: 0.8416, G Loss: 1.6079\n",
      "Epoch [47/50], Step [900/938], D Loss: 0.7089, G Loss: 2.9490\n",
      "Epoch [48/50], Step [100/938], D Loss: 0.9053, G Loss: 1.4996\n",
      "Epoch [48/50], Step [200/938], D Loss: 1.0287, G Loss: 1.4604\n",
      "Epoch [48/50], Step [300/938], D Loss: 0.6028, G Loss: 2.1126\n",
      "Epoch [48/50], Step [400/938], D Loss: 0.9577, G Loss: 1.5928\n",
      "Epoch [48/50], Step [500/938], D Loss: 1.0102, G Loss: 1.8619\n",
      "Epoch [48/50], Step [600/938], D Loss: 0.9977, G Loss: 2.3560\n",
      "Epoch [48/50], Step [700/938], D Loss: 0.8226, G Loss: 2.0931\n",
      "Epoch [48/50], Step [800/938], D Loss: 0.9048, G Loss: 2.2022\n",
      "Epoch [48/50], Step [900/938], D Loss: 0.9388, G Loss: 1.9466\n",
      "Epoch [49/50], Step [100/938], D Loss: 0.7791, G Loss: 2.2684\n",
      "Epoch [49/50], Step [200/938], D Loss: 0.7688, G Loss: 1.7390\n",
      "Epoch [49/50], Step [300/938], D Loss: 0.7479, G Loss: 2.2090\n",
      "Epoch [49/50], Step [400/938], D Loss: 0.7380, G Loss: 1.8822\n",
      "Epoch [49/50], Step [500/938], D Loss: 0.8542, G Loss: 1.8336\n",
      "Epoch [49/50], Step [600/938], D Loss: 0.9139, G Loss: 1.5882\n",
      "Epoch [49/50], Step [700/938], D Loss: 0.7279, G Loss: 2.2693\n",
      "Epoch [49/50], Step [800/938], D Loss: 1.0748, G Loss: 1.6087\n",
      "Epoch [49/50], Step [900/938], D Loss: 0.8534, G Loss: 2.0219\n",
      "Epoch [50/50], Step [100/938], D Loss: 0.7266, G Loss: 1.8598\n",
      "Epoch [50/50], Step [200/938], D Loss: 0.8046, G Loss: 1.4027\n",
      "Epoch [50/50], Step [300/938], D Loss: 0.8991, G Loss: 1.5003\n",
      "Epoch [50/50], Step [400/938], D Loss: 0.7042, G Loss: 1.9180\n",
      "Epoch [50/50], Step [500/938], D Loss: 0.8345, G Loss: 1.6326\n",
      "Epoch [50/50], Step [600/938], D Loss: 0.7101, G Loss: 1.5239\n",
      "Epoch [50/50], Step [700/938], D Loss: 0.9291, G Loss: 1.5933\n",
      "Epoch [50/50], Step [800/938], D Loss: 0.6873, G Loss: 2.0740\n",
      "Epoch [50/50], Step [900/938], D Loss: 1.0372, G Loss: 1.3214\n"
     ]
    }
   ],
   "source": [
    "def save_generated_images(epoch):\n",
    "    z = torch.randn(64, latent_dim).to(device)\n",
    "    fake_imgs = generator(z).cpu().detach()\n",
    "    fake_imgs = fake_imgs.view(-1, 28, 28)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(fake_imgs.size(0)):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(fake_imgs[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(f'gan_generated_epoch_{epoch + 1}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Run Training\n",
    "train_gan(num_epochs)\n",
    "\n",
    "# Save Models\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3be300-5a64-4d18-a898-a33dbf2a26af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
